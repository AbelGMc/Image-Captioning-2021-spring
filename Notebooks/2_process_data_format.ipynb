{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('labusr': conda)"
  },
  "interpreter": {
   "hash": "f3033dbcbae6bb642ed57e4081e4d44f6ee06ab71c363517056fcbe5febdb814"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "In this file, I convert features in `./data/feature.bin` to data from required in `captioning` module."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, array\n",
    "import numpy as np\n",
    "\n",
    "class BigFile:\n",
    "\n",
    "    def __init__(self, datadir):\n",
    "        self.nr_of_images, self.ndims = map(int, open(os.path.join(datadir,'shape.txt')).readline().split())\n",
    "        id_file = os.path.join(datadir, \"id.txt\")\n",
    "        self.names = open(id_file).read().strip().split()\n",
    "        assert(len(self.names) == self.nr_of_images)\n",
    "        self.name2index = dict(zip(self.names, range(self.nr_of_images)))\n",
    "        self.binary_file = os.path.join(datadir, \"feature.bin\")\n",
    "        print (\"[%s] %dx%d instances loaded from %s\" % (self.__class__.__name__, self.nr_of_images, self.ndims, datadir))\n",
    "\n",
    "\n",
    "    def read(self, requested, isname=True):\n",
    "        requested = set(requested)\n",
    "        if isname:\n",
    "            index_name_array = [(self.name2index[x], x) for x in requested if x in self.name2index]\n",
    "        else:\n",
    "            assert(min(requested)>=0)\n",
    "            assert(max(requested)<len(self.names))\n",
    "            index_name_array = [(x, self.names[x]) for x in requested]\n",
    "        if len(index_name_array) == 0:\n",
    "            return [], []\n",
    "       \n",
    "        index_name_array.sort(key=lambda v:v[0])\n",
    "        sorted_index = [x[0] for x in index_name_array]\n",
    "\n",
    "        nr_of_images = len(index_name_array)\n",
    "        vecs = [None] * nr_of_images\n",
    "        offset = np.float32(1).nbytes * self.ndims\n",
    "        \n",
    "        res = array.array('f')\n",
    "        fr = open(self.binary_file, 'rb')\n",
    "        fr.seek(index_name_array[0][0] * offset)\n",
    "        res.fromfile(fr, self.ndims)\n",
    "        previous = index_name_array[0][0]\n",
    " \n",
    "        for next in sorted_index[1:]:\n",
    "            move = (next-1-previous) * offset\n",
    "            #print next, move\n",
    "            fr.seek(move, 1)\n",
    "            res.fromfile(fr, self.ndims)\n",
    "            previous = next\n",
    "\n",
    "        fr.close()\n",
    "\n",
    "        return [x[0] for x in index_name_array], [ res[i*self.ndims:(i+1)*self.ndims].tolist() for i in range(nr_of_images) ]\n",
    "\n",
    "\n",
    "    def read_one(self, name):\n",
    "        renamed, vectors = self.read([name])\n",
    "        return vectors[0]    \n",
    "\n",
    "    def shape(self):\n",
    "        return [self.nr_of_images, self.ndims]\n",
    "\n",
    "\n",
    "class StreamFile:\n",
    "\n",
    "    def __init__(self, datadir):\n",
    "        self.feat_dir = datadir\n",
    "        self.nr_of_images, self.ndims = map(int, open(os.path.join(datadir,'shape.txt')).readline().split())\n",
    "        id_file = os.path.join(datadir, \"id.txt\")\n",
    "        self.names = open(id_file).read().strip().split()\n",
    "        assert(len(self.names) == self.nr_of_images)\n",
    "        self.name2index = dict(zip(self.names, range(self.nr_of_images)))\n",
    "        self.binary_file = os.path.join(datadir, \"feature.bin\")\n",
    "        print (\"[%s] %dx%d instances loaded from %s\" % (self.__class__.__name__, self.nr_of_images, self.ndims, datadir))\n",
    "        self.fr = None\n",
    "        self.current = 0\n",
    "    \n",
    "\n",
    "    def open(self):\n",
    "        self.fr = open(os.path.join(self.feat_dir,'feature.bin'), 'rb')\n",
    "        self.current = 0\n",
    "\n",
    "    def close(self):\n",
    "        if self.fr:\n",
    "            self.fr.close()\n",
    "            self.fr = None\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "        \n",
    "    def next(self):\n",
    "        if self.current >= self.nr_of_images:\n",
    "            self.close()\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            res = array.array('f')\n",
    "            res.fromfile(self.fr, self.ndims)\n",
    "            _id = self.names[self.current]\n",
    "            self.current += 1\n",
    "            return _id, res.tolist() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[BigFile] 8091x1024 instances loaded from ../data\n['1000268201_693b08cb0e', '1001773457_577c3a7d70', '1002674143_1b742ab4b8']\n8091\n----------------------------\n[0]\n1024\n"
     ]
    }
   ],
   "source": [
    "bigfile = BigFile('../data')\n",
    "print(bigfile.names[0:3])\n",
    "print(bigfile.nr_of_images)\n",
    "\n",
    "print('----------------------------')\n",
    "renamed, vectors = bigfile.read([bigfile.names[0]])\n",
    "print(renamed)\n",
    "print(len(vectors[0]))\n",
    "\n",
    "#     for name,vec in zip(renamed, vectors):\n",
    "#         print(name, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "name2index_file = open(\"../data/name2index.pkl\", \"wb\")\n",
    "pickle.dump(bigfile.name2index, name2index_file)\n",
    "name2index_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "len(bigfile.read([bigfile.names[0]])[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "deature_dir = '../data/feature'\n",
    "for name in bigfile.names:\n",
    "    renamed, vectors = bigfile.read([name]) # read() is modified to return [id, feature]\n",
    "    renamed, vectors = renamed[0], vectors[0]\n",
    "    np.save(os.path.join(deature_dir, str(renamed)+'.npy'), np.array(vectors,dtype=float))"
   ]
  }
 ]
}